\documentclass[a4paper,11pt,twoside]{article}
\usepackage[T1]{fontenc}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage{ngerman, eucal, mathrsfs, amsfonts, bbm, amsmath, amssymb, stmaryrd,graphicx, array, geometry, color, wrapfig}
\geometry{left=25mm, right=15mm, bottom=25mm}
\setlength{\parindent}{0em} 
\setlength{\headheight}{0em} 
\title{Machine Learning\\ Blatt 4}
\author{Markus Vieth, David Klopp, Christian Stricker}
\date{\today}
\input{../head/lstlisting.tex}
\begin{document}

\newcommand{\cor}[1]{\textcolor{red}{\textit{#1}}}
\maketitle
\cleardoublepage
\pagestyle{myheadings}
\markboth{Markus Vieth,  David Klopp, Christian Stricker}{Markus Vieth, David Klopp, Christian Stricker}

\newpage

\section*{Nr.1}
\begin{figure*}[h]
	\captionsetup[subfigure]{labelformat=empty}
	\centering
	\begin{subfigure}[t]{0.33\textwidth}
		\includegraphics[width=\textwidth]{images/Nr_1/d_1.png}
		\caption{Abbildung 1: d=1}
	\end{subfigure}%
	~
	\begin{subfigure}[t]{0.33\textwidth}
		\includegraphics[width=\textwidth]{images/Nr_1/d_2.png}
		\caption{Abbildung 2: d=2}
	\end{subfigure}%
	~
	\begin{subfigure}[t]{0.33\textwidth}
		\includegraphics[width=\textwidth]{images/Nr_1/d_3.png}
		\caption{Abbildung 3: d=3}
	\end{subfigure}
\end{figure*}

\begin{figure*}[h]
	\captionsetup[subfigure]{labelformat=empty}
	\centering
	\begin{subfigure}[t]{0.33\textwidth}
		\includegraphics[width=\textwidth]{images/Nr_1/d_4.png}
		\caption{Abbildung 4: d=4}
	\end{subfigure}%
	~
	\begin{subfigure}[t]{0.33\textwidth}
		\includegraphics[width=\textwidth]{images/Nr_1/d_5.png}
		\caption{Abbildung 5: d=5}
	\end{subfigure}%
	~
	\begin{subfigure}[t]{0.33\textwidth}
		\includegraphics[width=\textwidth]{images/Nr_1/d_10.png}
		\caption{Abbildung 6: d=10}
	\end{subfigure}
\end{figure*}

Die Grafiken verdeutlichen, dass der Bias zu Beginn sehr hoch ist, da der Sinus nicht durch eine Funktion ersten Gerades akkurat angenähert werden kann. Je höher man nun den Wert d, also den Grad des Polynoms zum annähern wählt, desto präziser kann die Sinus-Funktion approximiert werden und desto kleiner wird der Bias. Die Varianz lässt sich in den angegebenen Abbildungen nur schwer erkennen. In Abbildung 6 im Interval 0 bis 0.2, lässt sich eine leichte Streuung erkennen, die durch das Rauschen der Trainingsdaten verursacht worden sein könnte. Diese leichte Streuung kann allerdings auch durch den immer noch zu geringen Grad d des Polynoms entstanden sein.


\section*{Nr.2}


\section*{Nr.3}
\lstinputlisting[language = java]{code/Nr3.java}


\section*{Nr.4}


\section*{Nr.5}
\[accuracy = \frac{TP + TN}{TP + FP + FN + TN}\]
\[= \frac{TP}{P+N} + \frac{TN}{P+N}\]
\[= \frac{P \cdot TP}{P \cdot (P+N)} + \frac{N \cdot TN}{N \cdot (P +N)}\]
\[= \frac{TP}{P} \cdot \frac{P}{P+N} + \frac{TN}{N} \cdot \frac{N}{P+N} \]
\[= \frac{TP}{P} \cdot \frac{P}{P+N} + \frac{TN}{N} \cdot \left(1- \frac{P}{P+N}\right) \]
Mit A = $\frac{P}{P+N}$ folgt: 
\[accuracy = sensitivity \cdot A + specificity \cdot (1-A)\]

\end{document}